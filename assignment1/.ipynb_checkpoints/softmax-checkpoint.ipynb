{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.374266\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in* The weights W are initialized close to 0, so scores ~ 0, when you exponentiate, ~1, loss per example is near to -log(1/num_classes) = -log(1/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.162770 analytic: 1.162770, relative error: 7.110419e-08\n",
      "numerical: -0.179851 analytic: -0.179851, relative error: 3.556324e-07\n",
      "numerical: -1.965965 analytic: -1.965965, relative error: 4.287987e-08\n",
      "numerical: 1.193196 analytic: 1.193196, relative error: 4.835931e-08\n",
      "numerical: 1.810425 analytic: 1.810425, relative error: 2.713364e-08\n",
      "numerical: 0.298057 analytic: 0.298057, relative error: 1.395188e-07\n",
      "numerical: 2.646012 analytic: 2.646012, relative error: 3.254884e-08\n",
      "numerical: -1.906139 analytic: -1.906139, relative error: 3.491404e-08\n",
      "numerical: 1.516777 analytic: 1.516777, relative error: 2.365201e-08\n",
      "numerical: -2.559781 analytic: -2.559781, relative error: 6.459147e-09\n",
      "numerical: -1.606431 analytic: -1.606431, relative error: 2.888973e-08\n",
      "numerical: 3.392494 analytic: 3.392494, relative error: 2.618194e-08\n",
      "numerical: -1.567194 analytic: -1.567194, relative error: 6.399587e-09\n",
      "numerical: -0.708772 analytic: -0.708772, relative error: 1.850374e-08\n",
      "numerical: -0.504467 analytic: -0.504467, relative error: 4.352238e-08\n",
      "numerical: -2.453115 analytic: -2.453115, relative error: 1.368958e-08\n",
      "numerical: 1.041970 analytic: 1.041970, relative error: 2.703666e-08\n",
      "numerical: 1.910806 analytic: 1.910806, relative error: 3.686357e-08\n",
      "numerical: -1.959291 analytic: -1.959291, relative error: 3.678065e-08\n",
      "numerical: 1.598906 analytic: 1.598906, relative error: 2.381344e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.374266e+00 computed in 0.085114s\n",
      "vectorized loss: 2.374266e+00 computed in 0.097570s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 7.728679e+02\n",
      "iteration 100 / 1000: loss 2.836587e+02\n",
      "iteration 200 / 1000: loss 1.051894e+02\n",
      "iteration 300 / 1000: loss 3.981341e+01\n",
      "iteration 400 / 1000: loss 1.589363e+01\n",
      "iteration 500 / 1000: loss 7.142028e+00\n",
      "iteration 600 / 1000: loss 3.933646e+00\n",
      "iteration 700 / 1000: loss 2.696774e+00\n",
      "iteration 800 / 1000: loss 2.320707e+00\n",
      "iteration 900 / 1000: loss 2.192132e+00\n",
      "iteration 0 / 1000: loss 1.717619e+05\n",
      "iteration 100 / 1000: loss 2.302000e+00\n",
      "iteration 200 / 1000: loss 2.302343e+00\n",
      "iteration 300 / 1000: loss 2.303541e+00\n",
      "iteration 400 / 1000: loss 2.302205e+00\n",
      "iteration 500 / 1000: loss 2.302215e+00\n",
      "iteration 600 / 1000: loss 2.300750e+00\n",
      "iteration 700 / 1000: loss 2.301136e+00\n",
      "iteration 800 / 1000: loss 2.301563e+00\n",
      "iteration 900 / 1000: loss 2.301480e+00\n",
      "iteration 0 / 1000: loss 3.407514e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.110044e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.819960e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.664258e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.017214e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.190470e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.378882e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.544462e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.784588e+02\n",
      "iteration 100 / 1000: loss 1.831253e+02\n",
      "iteration 200 / 1000: loss 4.448705e+01\n",
      "iteration 300 / 1000: loss 1.202861e+01\n",
      "iteration 400 / 1000: loss 4.422412e+00\n",
      "iteration 500 / 1000: loss 2.584037e+00\n",
      "iteration 600 / 1000: loss 2.192441e+00\n",
      "iteration 700 / 1000: loss 2.059344e+00\n",
      "iteration 800 / 1000: loss 2.090921e+00\n",
      "iteration 900 / 1000: loss 2.069853e+00\n",
      "iteration 0 / 1000: loss 1.707075e+05\n",
      "iteration 100 / 1000: loss 2.303820e+00\n",
      "iteration 200 / 1000: loss 2.306628e+00\n",
      "iteration 300 / 1000: loss 2.310822e+00\n",
      "iteration 400 / 1000: loss 2.314320e+00\n",
      "iteration 500 / 1000: loss 2.311921e+00\n",
      "iteration 600 / 1000: loss 2.308165e+00\n",
      "iteration 700 / 1000: loss 2.310866e+00\n",
      "iteration 800 / 1000: loss 2.305905e+00\n",
      "iteration 900 / 1000: loss 2.309687e+00\n",
      "iteration 0 / 1000: loss 3.428055e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.099120e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.831227e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.557434e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.030276e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.194839e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.371127e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.525541e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.698081e+02\n",
      "iteration 100 / 1000: loss 1.161481e+02\n",
      "iteration 200 / 1000: loss 1.909305e+01\n",
      "iteration 300 / 1000: loss 4.669322e+00\n",
      "iteration 400 / 1000: loss 2.417811e+00\n",
      "iteration 500 / 1000: loss 2.159312e+00\n",
      "iteration 600 / 1000: loss 2.052448e+00\n",
      "iteration 700 / 1000: loss 2.058337e+00\n",
      "iteration 800 / 1000: loss 2.102419e+00\n",
      "iteration 900 / 1000: loss 2.106914e+00\n",
      "iteration 0 / 1000: loss 1.708064e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.428479e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.193010e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.885302e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.599130e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.032510e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.201007e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.351937e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.531119e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.738199e+02\n",
      "iteration 100 / 1000: loss 7.528739e+01\n",
      "iteration 200 / 1000: loss 8.940437e+00\n",
      "iteration 300 / 1000: loss 2.649790e+00\n",
      "iteration 400 / 1000: loss 2.141552e+00\n",
      "iteration 500 / 1000: loss 2.093788e+00\n",
      "iteration 600 / 1000: loss 2.090760e+00\n",
      "iteration 700 / 1000: loss 2.075954e+00\n",
      "iteration 800 / 1000: loss 2.044289e+00\n",
      "iteration 900 / 1000: loss 2.088678e+00\n",
      "iteration 0 / 1000: loss 1.726165e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.402045e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.149501e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.801682e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.658763e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.019380e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.205464e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.343222e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.538767e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.649151e+02\n",
      "iteration 100 / 1000: loss 4.817527e+01\n",
      "iteration 200 / 1000: loss 4.815812e+00\n",
      "iteration 300 / 1000: loss 2.257487e+00\n",
      "iteration 400 / 1000: loss 2.121453e+00\n",
      "iteration 500 / 1000: loss 2.151037e+00\n",
      "iteration 600 / 1000: loss 2.031033e+00\n",
      "iteration 700 / 1000: loss 2.136500e+00\n",
      "iteration 800 / 1000: loss 2.092591e+00\n",
      "iteration 900 / 1000: loss 2.109773e+00\n",
      "iteration 0 / 1000: loss 1.727592e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.381190e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.152327e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.699385e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.470810e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.025278e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.180120e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.381223e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.538956e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.780123e+02\n",
      "iteration 100 / 1000: loss 3.189242e+01\n",
      "iteration 200 / 1000: loss 3.283638e+00\n",
      "iteration 300 / 1000: loss 2.137686e+00\n",
      "iteration 400 / 1000: loss 2.100672e+00\n",
      "iteration 500 / 1000: loss 2.073547e+00\n",
      "iteration 600 / 1000: loss 2.066681e+00\n",
      "iteration 700 / 1000: loss 2.068166e+00\n",
      "iteration 800 / 1000: loss 2.009817e+00\n",
      "iteration 900 / 1000: loss 2.094104e+00\n",
      "iteration 0 / 1000: loss 1.707414e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.423318e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.140548e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.793754e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.504241e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.032046e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.190199e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.360137e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.535868e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.733260e+02\n",
      "iteration 100 / 1000: loss 2.096614e+01\n",
      "iteration 200 / 1000: loss 2.573255e+00\n",
      "iteration 300 / 1000: loss 2.092021e+00\n",
      "iteration 400 / 1000: loss 2.102574e+00\n",
      "iteration 500 / 1000: loss 2.132299e+00\n",
      "iteration 600 / 1000: loss 2.073957e+00\n",
      "iteration 700 / 1000: loss 2.064400e+00\n",
      "iteration 800 / 1000: loss 2.086655e+00\n",
      "iteration 900 / 1000: loss 2.094921e+00\n",
      "iteration 0 / 1000: loss 1.712019e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.420362e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.158263e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.878810e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.518865e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.016378e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.186742e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.382110e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.534286e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.675971e+02\n",
      "iteration 100 / 1000: loss 1.404954e+01\n",
      "iteration 200 / 1000: loss 2.278320e+00\n",
      "iteration 300 / 1000: loss 2.052971e+00\n",
      "iteration 400 / 1000: loss 2.149719e+00\n",
      "iteration 500 / 1000: loss 2.144408e+00\n",
      "iteration 600 / 1000: loss 2.087589e+00\n",
      "iteration 700 / 1000: loss 2.092247e+00\n",
      "iteration 800 / 1000: loss 2.070496e+00\n",
      "iteration 900 / 1000: loss 2.116683e+00\n",
      "iteration 0 / 1000: loss 1.725470e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.407096e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.133045e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.829568e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.554895e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.020828e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.201155e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.376002e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.537145e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.751445e+02\n",
      "iteration 100 / 1000: loss 9.665167e+00\n",
      "iteration 200 / 1000: loss 2.151379e+00\n",
      "iteration 300 / 1000: loss 2.056688e+00\n",
      "iteration 400 / 1000: loss 2.110976e+00\n",
      "iteration 500 / 1000: loss 2.068186e+00\n",
      "iteration 600 / 1000: loss 2.090915e+00\n",
      "iteration 700 / 1000: loss 2.087757e+00\n",
      "iteration 800 / 1000: loss 2.091395e+00\n",
      "iteration 900 / 1000: loss 2.076706e+00\n",
      "iteration 0 / 1000: loss 1.716723e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.434966e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.175244e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.795860e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.614417e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.017713e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.212626e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.353221e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.541333e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 7.739926e+02\n",
      "iteration 100 / 1000: loss 6.900872e+00\n",
      "iteration 200 / 1000: loss 2.108912e+00\n",
      "iteration 300 / 1000: loss 2.088174e+00\n",
      "iteration 400 / 1000: loss 2.131584e+00\n",
      "iteration 500 / 1000: loss 2.090846e+00\n",
      "iteration 600 / 1000: loss 2.077949e+00\n",
      "iteration 700 / 1000: loss 2.083708e+00\n",
      "iteration 800 / 1000: loss 2.067672e+00\n",
      "iteration 900 / 1000: loss 2.053814e+00\n",
      "iteration 0 / 1000: loss 1.709538e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 3.415692e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 5.120302e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 6.789832e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 8.543440e+05\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.040028e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.199875e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.360166e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "iteration 0 / 1000: loss 1.550553e+06\n",
      "iteration 100 / 1000: loss nan\n",
      "iteration 200 / 1000: loss nan\n",
      "iteration 300 / 1000: loss nan\n",
      "iteration 400 / 1000: loss nan\n",
      "iteration 500 / 1000: loss nan\n",
      "iteration 600 / 1000: loss nan\n",
      "iteration 700 / 1000: loss nan\n",
      "iteration 800 / 1000: loss nan\n",
      "iteration 900 / 1000: loss nan\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.333388 val accuracy: 0.347000\n",
      "lr 1.000000e-07 reg 1.115556e+07 train accuracy: 0.219061 val accuracy: 0.240000\n",
      "lr 1.000000e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 5.000000e+04 train accuracy: 0.321939 val accuracy: 0.336000\n",
      "lr 1.444444e-07 reg 1.115556e+07 train accuracy: 0.180388 val accuracy: 0.170000\n",
      "lr 1.444444e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.444444e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 5.000000e+04 train accuracy: 0.335816 val accuracy: 0.346000\n",
      "lr 1.888889e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.888889e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 5.000000e+04 train accuracy: 0.330041 val accuracy: 0.347000\n",
      "lr 2.333333e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.333333e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 5.000000e+04 train accuracy: 0.324327 val accuracy: 0.350000\n",
      "lr 2.777778e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.777778e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 5.000000e+04 train accuracy: 0.326102 val accuracy: 0.335000\n",
      "lr 3.222222e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.222222e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 5.000000e+04 train accuracy: 0.333755 val accuracy: 0.349000\n",
      "lr 3.666667e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.666667e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 5.000000e+04 train accuracy: 0.324265 val accuracy: 0.335000\n",
      "lr 4.111111e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.111111e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 5.000000e+04 train accuracy: 0.316143 val accuracy: 0.342000\n",
      "lr 4.555556e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.555556e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.323020 val accuracy: 0.331000\n",
      "lr 5.000000e-07 reg 1.115556e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 2.226111e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 3.336667e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 4.447222e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 5.557778e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 6.668333e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 7.778889e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 8.889444e+07 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-07 reg 1.000000e+08 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.350000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "lr_start = learning_rates[0]\n",
    "lr_end = learning_rates[1]\n",
    "\n",
    "reg_start = regularization_strengths[0]\n",
    "reg_end = regularization_strengths[1]\n",
    "\n",
    "num_tries = 10\n",
    "\n",
    "for lr in np.linspace(lr_start, lr_end, num_tries):\n",
    "    for reg in np.linspace(reg_start, reg_end, num_tries):\n",
    "        softmax = Softmax()\n",
    "        loss_history = softmax.train(X_train, y_train, lr,reg, \n",
    "                                     num_iters=1000, verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_acc = np.mean(y_train_pred == y_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val_pred == y_val)\n",
    "        \n",
    "        results[(lr, reg)] = (train_acc, val_acc)\n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "        \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.345000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzQAAAIUCAYAAADMqWn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPoVtt/60VmkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9295610750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
